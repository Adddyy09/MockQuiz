export const quizData = [
    // Part 1: True or False
    { id: 1, type: 'tf', question: "An AI speaker is a typical use case for speech processing.", options: ["True", "False"], answer: "True", explanation: "AI speakers rely heavily on speech recognition and processing to function." },
    { id: 2, type: 'tf', question: "Huawei's all-scenario AI solutions can be deployed in all kinds of environments, including public cloud, private clouds, various forms of edge computing, industrial IoT devices, and consumer devices.", options: ["True", "False"], answer: "True", explanation: "Huawei promotes its AI solutions as being deployable across a wide range of scenarios." },
    { id: 3, type: 'tf', question: "Typically, in a network that uses sigmoid, the gradient decreases to 0 within five layers, making the network difficult to train.", options: ["True", "False"], answer: "True", explanation: "This describes the vanishing gradient problem, which is prominent with the sigmoid activation function in deep networks." },
    { id: 4, type: 'tf', question: "When you train a deep neural network with the softsign function, if the network is very deep, an increasing number of back-propagated gradients fall into the saturation area, making the gradient module smaller and finally close to 0, and the weight cannot be updated.", options: ["True", "False"], answer: "True", explanation: "The softsign function, similar to sigmoid and tanh, also suffers from the vanishing gradient problem in very deep networks." },
    { id: 5, type: 'tf', question: "The tanh function can effectively solve the vanishing gradient problem.", options: ["True", "False"], answer: "False", explanation: "While tanh is often better than sigmoid, it still suffers from the vanishing gradient problem because its gradients are also between 0 and 1." },
    { id: 6, type: 'tf', question: "L1 regularization is referred to as weight decay.", options: ["True", "False"], answer: "False", explanation: "L2 regularization is referred to as weight decay. L1 regularization leads to sparse weights." },
    { id: 7, type: 'tf', question: "If the hyperparameter search space is large, random search is better than grid search.", options: ["True", "False"], answer: "True", explanation: "Random search is often more efficient than grid search in high-dimensional spaces because it's less likely to waste trials on unimportant parameters." },
    { id: 8, type: 'tf', question: "Assuming a dataset contains the areas and prices of 21,613 housing units in a city, you can use a classification model to predict the prices of other housing units in the city.", options: ["True", "False"], answer: "False", explanation: "Predicting a continuous value like price is a regression task, not a classification task." },
    { id: 9, type: 'tf', question: "The emergence of foundation models represents a shift from weak AI to general AI.", options: ["True", "False"], answer: "False", explanation: "Foundation models are a significant step, but they are still considered a form of narrow or weak AI, not Artificial General Intelligence (AGI)." },
    { id: 10, type: 'tf', question: "Batch normalization is a technique used in deep learning to optimize training. It reduces the dependence on initial weights and learning rates, resulting in improved model performance.", options: ["True", "False"], answer: "True", explanation: "Batch normalization helps stabilize and accelerate the training process by normalizing the inputs to each layer." },
    { id: 11, type: 'tf', question: "In regression tasks, the most commonly used loss function is cross entropy cost.", options: ["True", "False"], answer: "False", explanation: "Cross-entropy is used for classification tasks. Regression tasks typically use loss functions like Mean Squared Error (MSE) or Mean Absolute Error (MAE)." },
    { id: 12, type: 'tf', question: "In data parallelism, the data of each training batch is split into N parts, forward propagation is performed across computing units, and the loss value calculated by each card is aggregated on the master node.", options: ["True", "False"], answer: "False", explanation: "In data parallelism, gradients are aggregated, not loss values. Each device computes its own loss and gradients, and the gradients are then synchronized." },
    { id: 13, type: 'tf', question: "In pipeline parallelism, the parameters of each layer of a model are split into multiple parts and distributed to multiple computing units.", options: ["True", "False"], answer: "False", explanation: "This describes tensor parallelism. Pipeline parallelism splits model layers across different devices, not the parameters within a layer." },
    { id: 14, type: 'tf', question: "Model fine-tuning is to adjust the model structure and the number of trainable parameters based on the pre-trained model, and to use more diverse data for training.", options: ["True", "False"], answer: "False", explanation: "Fine-tuning typically involves training a pre-trained model on a new, specific dataset, often with a smaller learning rate. It doesn't usually involve changing the model structure." },
    { id: 15, type: 'tf', question: "The core idea of adapter tuning is to add adapter modules to specific layers of a pre-trained model. These modules usually consist of two small neural networks: one for dimensionality reduction and the other for augmentation.", options: ["True", "False"], answer: "True", explanation: "Adapter tuning is a parameter-efficient fine-tuning (PEFT) method that freezes the pre-trained model and only trains small, newly added adapter modules. The two networks perform dimensionality reduction and then augmentation (restoring the original dimension)." },

    // Part 2: Single-Answer Multiple Choice
    { id: 16, type: 'mc', question: "Without considering any regularization terms, the support vectors of a support vector machine (SVM) are composed of:", options: ["A. Points on the separating hyperplane", "B. The points farthest from the separating hyperplane", "C. The points closest to the separating hyperplane", "D. Points of a certain type"], answer: "C. The points closest to the separating hyperplane", explanation: "Support vectors are the data points that lie closest to the decision surface (hyperplane); they are the most difficult to classify and directly influence the position of the hyperplane." },
    { id: 17, type: 'mc', question: "During neural network training, which of the following values is continuously updated using the gradient descent method to minimize the loss function?", options: ["A. Hyperparameters", "B. Feature value", "C. Number of samples", "D. Parameters"], answer: "D. Parameters", explanation: "Gradient descent is an optimization algorithm used to update the model's parameters (weights and biases) to minimize the loss." },
    { id: 18, type: 'mc', question: "The ReLU function is commonly used in deep learning neural networks. Which of the following is the value range of this function?", options: ["A. [0, +∞)", "B. [0, 1]", "C. [-1, 1]", "D. [-1, 0]"], answer: "A. [0, +∞)", explanation: "The Rectified Linear Unit (ReLU) function is defined as f(x) = max(0, x), so its output is always non-negative." },
    { id: 19, type: 'mc', question: "The sigmoid activation function is monotonic and continuous, has bounded outputs, and makes the network easy to converge. It was popular for a while, but when the network is deep, which one of these problems is sigmoid associated with?", options: ["A. Gradient reduction", "B. Vanishing gradient", "C. XOR Problem", "D. Overfitting"], answer: "B. Vanishing gradient", explanation: "The derivative of the sigmoid function is small (max 0.25), which can cause gradients to shrink to near zero during backpropagation in deep networks." },
    { id: 20, type: 'mc', question: "Which of the following functions can be used to alleviate the vanishing gradient problem?", options: ["A. Sigmoid", "B. Tanh", "C. Softsign", "D. ReLU"], answer: "D. ReLU", explanation: "ReLU and its variants (like Leaky ReLU) help alleviate the vanishing gradient problem because their derivative is 1 for positive inputs, preventing the gradient from shrinking." },
    { id:21, type: 'mc', question: "If a 32x32 image is input and a 5x5 kernel is used for convolution with the stride of 1, the size of the output image is:", options: ["A. 28x28", "B. 29x29", "C. 28x23", "D. 23x23"], answer: "A. 28x28", explanation: "Output size = (Input size - Kernel size) / Stride + 1. So, (32 - 5) / 1 + 1 = 28. The output is 28x28." },
    { id: 22, type: 'mc', question: "A manufacturer wants to produce virtual assistants for hospital use. For virtual assistants, which of the following technologies is used for voiceprint recognition?", options: ["A. Speech recognition and processing", "B. Image recognition and processing", "C. Expert system and knowledge graph", "D. Image generation and enhancement"], answer: "A. Speech recognition and processing", explanation: "Voiceprint recognition is a key part of speech processing, used for identifying a speaker." },
    { id: 23, type: 'mc', question: "Which of the following is NOT a use case for natural language processing?", options: ["A. Public opinion analysis", "B. Machine translation", "C. Text classification", "D. Image recognition"], answer: "D. Image recognition", explanation: "Image recognition is a task for computer vision, not natural language processing." },
    { id: 24, type: 'mc', question: "Which of the following is NOT a recurrent neural network?", options: ["A. RNN", "B. LSTM", "C. CNN", "D. GRU"], answer: "C. CNN", explanation: "CNN (Convolutional Neural Network) is primarily used for image processing. RNN, LSTM, and GRU are all types of recurrent networks designed for sequential data." },
    { id: 25, type: 'mc', question: "Within the broad landscape of large language models built on the Transformer architecture, which roadmap does GPT belong to?", options: ["A. Encoder-Only", "B. Decoder-Only", "C. Encoder-Decoder", "D. Decoder-Encoder"], answer: "B. Decoder-Only", explanation: "GPT (Generative Pre-trained Transformer) models use a decoder-only architecture, making them excellent for text generation tasks." },
    { id: 26, type: 'mc', question: "Which of the following statements about different optimizers is false?", options: ["A. RMSprop solves the problem of the Adagrad optimizer ending too early.", "B. Compared with RMSprop, Adagrad is more sensitive to gradient changes.", "C. Both Adagrad and RMSprop can set adaptive learning rate for parameters.", "D. Adam requires bias correction for initial iterations."], answer: "B. Compared with RMSprop, Adagrad is more sensitive to gradient changes.", explanation: "Adagrad accumulates all past squared gradients, which can cause the learning rate to become too small. RMSprop uses an exponentially decaying average, making it less sensitive to early gradients and more adaptive to recent ones." },
    { id: 27, type: 'mc', question: "Which of the following statements is true about classification models and regression models in machine learning?", options: ["A. For regression problems, the output variables are discrete values. For classification problems, the output variables are continuous.", "B. The most commonly used indicators for evaluating regression and classification problems are the accuracy and the recall rate.", "C. Overfitting may occur in both regression and classification problems.", "D. Logistic regression is a typical regression model."], answer: "C. Overfitting may occur in both regression and classification problems.", explanation: "Overfitting is a general problem in machine learning where a model learns the training data too well, and it can happen in both classification and regression." },
    { id: 28, type: 'mc', question: "Which of the following statements about support vector machines (SVMs) is false?", options: ["A. SVMs are classification models. Their basic model is the linear classifier that maximizes the width of the gap between the two categories in the feature space.", "B. SVMs also have a kernel trick, which makes them non-linear classifiers.", "C. In the case of linear inseparability, non-linear mapping algorithms are used to convert the linearly inseparable samples of low-dimensional input space into samples of high-dimensional feature space. In this way, samples become linearly separable.", "D. SVMs only apply to linear classification."], answer: "D. SVMs only apply to linear classification.", explanation: "This is false. Using the 'kernel trick', SVMs can efficiently perform non-linear classification." },
    { id: 29, type: 'mc', question: "Which of the following is NOT an AI deep learning framework?", options: ["A. TensorFlow", "B. Scikit-learn", "C. PyTorch", "D. Caffe"], answer: "B. Scikit-learn", explanation: "Scikit-learn is a general-purpose machine learning library but is not considered a deep learning framework like TensorFlow, PyTorch, or Caffe." },
    { id: 30, type: 'mc', question: "Which of the following are used in TensorFlow to describe the computation process?", options: ["A. Parameter", "B. Session", "C. Data flow", "D. Tensor"], answer: "C. Data flow", explanation: "TensorFlow represents computations as data flow graphs, where nodes are operations and edges are tensors." },
    { id: 31, type: 'mc', question: "If a 32x32 feature map is processed through a pooling layer with a stride of 2 and a 2x2 filter, the size of the output map is:", options: ["A. 30x30", "B. 16x16", "C. 29x29", "D. 23x23"], answer: "B. 16x16", explanation: "With a stride of 2, a 2x2 pooling filter will halve the dimensions of the feature map. 32 / 2 = 16. The output is 16x16." },
    { id: 32, type: 'mc', question: "Which of the following is NOT a characteristic of PyTorch?", options: ["A. Python first", "B. Easy to debug", "C. Extensive ecosystem", "D. Default use of static maps"], answer: "D. Default use of static maps", explanation: "PyTorch is known for its use of dynamic computation graphs (eager execution), which makes it more flexible and easier to debug. TensorFlow 1.x used static graphs by default." },
    { id: 33, type: 'mc', question: "Which of the following is not an advanced AI application?", options: ["A. Knowledge graph", "B. Quantum computing", "C. SDN", "D. Intelligent driving"], answer: "C. SDN", explanation: "SDN (Software-Defined Networking) is a network architecture approach, not an AI application. The others are all advanced fields related to or utilizing AI." },
    { id: 34, type: 'mc', question: "Which of the following statements is true about the training of large models?", options: ["A. Foundation models cannot be directly deployed.", "B. The pre-training of large models requires a large amount of high-quality data.", "C. Unlabeled data used in the pre-training phase of large models must be used in the fine-tuning phase.", "D. Large models have emergent abilities, and therefore they do not need to be aligned with human values."], answer: "B. The pre-training of large models requires a large amount of high-quality data.", explanation: "The performance of large models is heavily dependent on the massive scale and quality of the data they are trained on." },
    { id: 35, type: 'mc', question: "Tensor parallelism is a common method for parallel training of large models. Which of the following types of model data is split to different devices?", options: ["A. Gradient", "B. Optimizer states", "C. Input data", "D. Model parameters"], answer: "D. Model parameters", explanation: "In tensor parallelism, the model's weight matrices (parameters) are split across multiple devices, and each device computes only a part of the matrix multiplication." },

    // Part 3: Multiple-Answer Questions
    { id: 36, type: 'mc', multi: true, question: "What are the main schools of thought in AI?", options: ["A. Symbolism", "B. Illusionism", "C. Actionism", "D. Connectionism"], answer_set: ["A. Symbolism", "C. Actionism", "D. Connectionism"], explanation: "The main historical schools of thought in AI are Symbolism (logic-based), Connectionism (neural network-based), and Actionism/Behaviorism (action-based)." },
    { id: 37, type: 'mc', multi: true, question: "What are the layers used in Transformer to capture time dependencies within a sequence?", options: ["A. Residual layer", "B. Self-attention layer", "C. Positional encoding", "D. Feed-forward network layer"], answer_set: ["B. Self-attention layer", "C. Positional encoding"], explanation: "Self-attention allows the model to weigh the importance of different words in the sequence, and positional encoding provides information about the position of words, both capturing dependencies." },
    { id: 38, type: 'mc', multi: true, question: "During the calculation of the attention mechanism, which of the following are required for similarity calculation to obtain the attention score?", options: ["A. Query", "B. Weight", "C. Value", "D. Key"], answer_set: ["A. Query", "D. Key"], explanation: "The attention score is calculated based on the similarity between the Query (Q) of the current token and the Key (K) of all other tokens in the sequence. The Value (V) is used later to compute the output." },
    { id: 39, type: 'mc', multi: true, question: "Which of the following are common evaluation metrics for classification tasks?", options: ["A. Accuracy", "B. Precision", "C. Recall", "D. Mean squared error"], answer_set: ["A. Accuracy", "B. Precision", "C. Recall"], explanation: "Accuracy, Precision, and Recall are standard classification metrics. Mean Squared Error (MSE) is used for regression tasks." },
    { id: 40, type: 'mc', multi: true, question: "Which of the following hyperparameters may be defined during model training?", options: ["A. Batch size", "B. Learning rate", "C. Optimizer type", "D. Loss value"], answer_set: ["A. Batch size", "B. Learning rate", "C. Optimizer type"], explanation: "Batch size, learning rate, and the choice of optimizer are all hyperparameters set by the user before training. The loss value is a result of the training process, not a hyperparameter." },
    { id: 41, type: 'mc', multi: true, question: "Optimizers are often used to accelerate training of deep learning neural networks. Which of the following are deep learning optimizers?", options: ["A. Momentum", "B. Adagrad", "C. RMSprop", "D. Adam"], answer_set: ["A. Momentum", "B. Adagrad", "C. RMSprop", "D. Adam"], explanation: "All of these are popular optimization algorithms used in deep learning to update model weights." },
    { id: 42, type: 'mc', multi: true, question: "Which of the following statements about some of the subfields of AI are true?", options: ["A. Computer vision is a science that studies how to make computers 'see' things.", "B. Speech processing is a general term for different speech processing methods and techniques.", "C. Natural language processing studies how to use computer technology to understand and use human language.", "D. Autonomous driving does not require speech processing or computer vision."], answer_set: ["A. Computer vision is a science that studies how to make computers 'see' things.", "B. Speech processing is a general term for different speech processing methods and techniques.", "C. Natural language processing studies how to use computer technology to understand and use human language."], explanation: "A, B, and C correctly define their respective fields. D is false, as autonomous driving heavily relies on computer vision (to see the road) and can use speech processing (for voice commands)." },
    { id: 43, type: 'mc', multi: true, question: "Which of the following are common activation functions of neural networks?", options: ["A. Dropout", "B. Sigmoid", "C. tanh", "D. Leaky ReLU"], answer_set: ["B. Sigmoid", "C. tanh", "D. Leaky ReLU"], explanation: "Sigmoid, tanh, and Leaky ReLU are all activation functions. Dropout is a regularization technique, not an activation function." },
    { id: 44, type: 'mc', multi: true, question: "Which of the following comprise the Adam optimizer?", options: ["A. Momentum", "B. Adagrad", "C. RMSprop", "D. Nesterov"], answer_set: ["A. Momentum", "C. RMSprop"], explanation: "Adam (Adaptive Moment Estimation) combines the ideas of Momentum (which uses a moving average of the gradient) and RMSprop (which uses a moving average of the squared gradient)." },
    { id: 45, type: 'mc', multi: true, question: "Which of the following statements about datasets are true?", options: ["A. A dataset is a collection of data used in machine learning tasks. Each piece of data is called a sample.", "B. Events or attributes that reflect the particular performance or nature of a sample are called features.", "C. A training set is a dataset used in the training process, where each sample is referred to as a training sample.", "D. Learning (or training) is the process of building a model from data."], answer_set: ["A. A dataset is a collection of data used in machine learning tasks. Each piece of data is called a sample.", "B. Events or attributes that reflect the particular performance or nature of a sample are called features.", "C. A training set is a dataset used in the training process, where each sample is referred to as a training sample.", "D. Learning (or training) is the process of building a model from data."], explanation: "All of these statements are fundamental and correct definitions in the context of machine learning." },
    { id: 46, type: 'mc', multi: true, question: "Which of the following statements about data preprocessing are true?", options: ["A. Data cleansing is a process of filling in missing values, as well as detecting and eliminating noise and exceptions.", "B. Data dimension reduction aims to simplify data attributes and avoid the curse of dimensionality.", "C. Data standardization aims to reduce noise data and improve model accuracy by standardizing data.", "D. Machine learning tasks generate results in the form of model outputs. Therefore, model training is more important than data preprocessing."], answer_set: ["A. Data cleansing is a process of filling in missing values, as well as detecting and eliminating noise and exceptions.", "B. Data dimension reduction aims to simplify data attributes and avoid the curse of dimensionality.", "C. Data standardization aims to reduce noise data and improve model accuracy by standardizing data."], explanation: "A, B, and C are all valid aspects of data preprocessing. D is false; data preprocessing is critically important ('garbage in, garbage out')." },
    { id: 47, type: 'mc', multi: true, question: "Which of the following statements about model parameters and hyperparameters are true?", options: ["A. Models contain both parameters and hyperparameters.", "B. Hyperparameters are automatically learned by models.", "C. Hyperparameters are manually set.", "D. Hyperparameters can be used to control training."], answer_set: ["A. Models contain both parameters and hyperparameters.", "C. Hyperparameters are manually set.", "D. Hyperparameters can be used to control training."], explanation: "Parameters are learned during training, while hyperparameters are set before training to control the learning process. B is false." },
    { id: 48, type: 'mc', multi: true, question: "Data augmentation can improve model robustness and avoid overfitting. Which of the following are data augmentation methods?", options: ["A. Batch normalization", "B. Image contrast change", "C. Image noise adding", "D. Random crop"], answer_set: ["B. Image contrast change", "C. Image noise adding", "D. Random crop"], explanation: "Changing contrast, adding noise, and random cropping are all common techniques for image data augmentation. Batch normalization is a training optimization technique." },
    { id: 49, type: 'mc', multi: true, question: "The softmax function is frequently used in classification tasks. Which of the following statements about it are true?", options: ["A. The softmax function is also called the normalized exponential function.", "B. The softmax regression model is an algorithm for solving binary classification regression problems.", "C. The softmax function is the generalization of the binary classification function sigmoid.", "D. The softmax function is usually used together with the cross-entropy loss function."], answer_set: ["A. The softmax function is also called the normalized exponential function.", "C. The softmax function is the generalization of the binary classification function sigmoid.", "D. The softmax function is usually used together with the cross-entropy loss function."], explanation: "A, C, and D are correct statements. B is incorrect because softmax is for multi-class classification, not binary regression." },
    { id: 50, type: 'mc', multi: true, question: "Which of the following are characteristics of eager execution in TensorFlow 2.x?", options: ["A. High performance", "B. Deployability", "C. Intuitiveness", "D. Flexibility"], answer_set: ["C. Intuitiveness", "D. Flexibility"], explanation: "Eager execution (like in PyTorch) makes code more intuitive and flexible for debugging and development. For high performance and deployment, TensorFlow often converts the model to a static graph using `tf.function`." },
    { id: 51, type: 'mc', multi: true, question: "The process of developing AI applications includes:", options: ["A. Data preprocessing", "B. Modeling", "C. Model training", "D. Model deployment"], answer_set: ["A. Data preprocessing", "B. Modeling", "C. Model training", "D. Model deployment"], explanation: "All listed steps are integral parts of the end-to-end AI application development lifecycle." },
    { id: 52, type: 'mc', multi: true, question: "Which of the following statements about large models are true?", options: ["A. A large amount of data is required for training.", "B. The model parameter scale is large.", "C. Training data does not need to be cleansed because this helps models learn how to process error information.", "D. The generalization capability of large models is stronger than that of small models."], answer_set: ["A. A large amount of data is required for training.", "B. The model parameter scale is large.", "D. The generalization capability of large models is stronger than that of small models."], explanation: "Large models are defined by their large parameter counts and data requirements, which leads to strong generalization. C is false; data quality is extremely important." },
    { id: 53, type: 'mc', multi: true, question: "Which of the following fields can large model technologies be applied to?", options: ["A. Autonomous driving", "B. Robots", "C. Translation", "D. Finance"], answer_set: ["A. Autonomous driving", "B. Robots", "C. Translation", "D. Finance"], explanation: "Large models have broad applicability and are being integrated into all of these fields for tasks like scene understanding, human-robot interaction, machine translation, and financial analysis." },
    { id: 54, type: 'mc', multi: true, question: "Which of the following fine-tuning methods do not change the Transformer block structure of a large model?", options: ["A. Adapter tuning", "B. Prefix tuning", "C. Prompt tuning", "D. LoRA"], answer_set: ["B. Prefix tuning", "C. Prompt tuning"], explanation: "All of these are Parameter-Efficient Fine-Tuning (PEFT) methods that work by adding a small number of trainable parameters and freezing the original large model, thus not changing its core structure." },
    { id: 55, type: 'mc', multi: true, question: "Which of the following are reasons for evaluating large models?", options: ["A. Improving the accuracy of model outputs", "B. Evaluating the capabilities of large models", "C. Providing reference for enterprises to select models", "D. Aligning large models with human values"], answer_set: ["B. Evaluating the capabilities of large models", "C. Providing reference for enterprises to select models"], explanation: "Evaluation is crucial for all these reasons: to measure and improve performance, understand capabilities, compare models for selection, and ensure they are safe and aligned with human intent." },

    // Part 4: Fill-in-the-Blank (Converted to MC)
    { id: 56, type: 'mc', question: "On a convolution layer of a convolutional neural network, if there are 128 3x3 convolution kernels and the size of the input feature map is 28x28x64, the depth of a convolution kernel is (_____).", options: ["A. 3", "B. 28", "C. 64", "D. 128"], answer: "C. 64", explanation: "The depth of a convolution kernel must match the depth of the input feature map it is convolving. In this case, the input depth is 64." },
    { id: 57, type: 'mc', question: "CNN stands for (_____).", options: ["A. Complex Neural Network", "B. Convolutional Neural Network", "C. Computerized Neural Network", "D. Connected Neural Network"], answer: "B. Convolutional Neural Network", explanation: "CNN is the standard abbreviation for Convolutional Neural Network." },
    { id: 58, type: 'mc', question: "The (_____) algorithm is based on the idea that if most of a sample's k nearest neighbors in the feature space belong to the same class, the sample also belongs to that class.", options: ["A. SVM", "B. PCA", "C. k-Means", "D. KNN"], answer: "D. KNN", explanation: "This describes the k-Nearest Neighbors (k-NN) algorithm, a simple and effective classification method." },
    { id: 59, type: 'mc', question: "In multi-class tasks, the sum of all output probability values of the Softmax function is (_____).", options: ["A. 0", "B. 1", "C. 100", "D. N (number of classes)"], answer: "B. 1", explanation: "The Softmax function converts a vector of numbers into a probability distribution, where the probabilities of each value are proportional to the relative scale of each value in the vector, and the sum is always 1." },
    { id: 60, type: 'mc', question: "In k-fold cross-validation, the existing data is divided into (_____) groups.", options: ["A. 2", "B. k", "C. N (number of samples)", "D. 10"], answer: "B. k", explanation: "The 'k' in k-fold cross-validation refers to the number of groups (or folds) the data is split into." }
];